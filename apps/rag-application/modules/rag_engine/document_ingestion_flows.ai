contract flow "rag_engine.initialize_defaults":
  input:
  output:
    status is text

contract flow "rag_engine.refresh_index_summary":
  input:
  output:
    status is text

contract flow "rag_engine.ensure_projects":
  input:
  output:
    status is text

contract flow "rag_engine.refresh_project_stats":
  input:
  output:
    status is text

flow "ingest_web_source": requires true
  let init_status is call flow "rag_engine.initialize_defaults":
    input:
    output:
      status

  let projects_status is call flow "rag_engine.ensure_projects":
    input:
    output:
      status

  set state.request_url is ""
  try:
    let requested_web_url is input.web_url
    if requested_web_url is not "":
      set state.request_url is requested_web_url
  with catch err:
    set state.request_url is state.request_url

  try:
    let requested_url is input.url
    if state.request_url is "":
      if requested_url is not "":
        set state.request_url is requested_url
  with catch err:
    set state.request_url is state.request_url

  if state.request_url is "":
    delete "Notice" where true
    create "Notice" with map:
      "id" is "current"
      "message" is "Provide a URL before adding a web source."
    as notice_missing_web_url
    return map:
      "status" is "missing_url"

  set state.web_upload_id is "web::" + state.request_url
  set state.web_source_name is state.request_url
  set state.web_selected is true
  set state.web_weight is 1
  set state.web_tags is list: "general"
  set state.web_primary_tag is "general"

  find "DocumentLibrary" where upload_id is state.web_upload_id
  let existing_doc_count is list length of documentlibrary_results
  if existing_doc_count is greater than 0:
    let existing_doc is list get documentlibrary_results at 0
    set state.web_source_name is existing_doc.source_name
    set state.web_selected is existing_doc.selected
    set state.web_weight is existing_doc.weight
    set state.web_tags is existing_doc.tags
    set state.web_primary_tag is existing_doc.primary_tag

  delete "DocumentLibrary" where upload_id is state.web_upload_id
  create "DocumentLibrary" with map:
    "id" is state.web_upload_id
    "upload_id" is state.web_upload_id
    "document_id" is state.web_upload_id
    "source_name" is state.web_source_name
    "source_type" is "WEB"
    "status" is "indexing"
    "ingestion_stage" is "fetching"
    "progress_percent" is 30
    "quality" is "pending"
    "selected" is state.web_selected
    "status_label" is "Fetching â€¢ WEB"
    "scope_label" is state.web_primary_tag + " â€¢ priority"
    "weight" is state.web_weight
    "tags" is state.web_tags
    "primary_tag" is state.web_primary_tag
    "quality_note" is "Fetching web page content."
    "last_error" is ""
  as web_fetching_row

  set state.web_status is 0
  set state.web_body is ""
  set state.web_error is ""

  try:
    let web_response is fetch web page:
      url is state.request_url

    try:
      let response_status is web_response.status
      set state.web_status is response_status
    with catch err:
      set state.web_status is state.web_status

    try:
      let response_body is web_response.body
      if response_body is not "":
        set state.web_body is response_body
    with catch err:
      set state.web_body is state.web_body
  with catch err:
    set state.web_error is "web_fetch_failed"

  set state.web_ok is true
  if state.web_error is not "":
    set state.web_ok is false
  if state.web_status is not 200:
    set state.web_ok is false
  if state.web_body is "":
    set state.web_ok is false

  if state.web_ok is false:
    set state.web_error_note is "Web fetch failed. Check URL accessibility and try again."
    if state.web_status is not 200:
      set state.web_error_note is "Web fetch returned non-200 status. Use a reachable public URL."
    if state.web_body is "":
      set state.web_error_note is "Web fetch returned no text body. Try a text-rich page."

    delete "DocumentLibrary" where upload_id is state.web_upload_id
    create "DocumentLibrary" with map:
      "id" is state.web_upload_id
      "upload_id" is state.web_upload_id
      "document_id" is state.web_upload_id
      "source_name" is state.web_source_name
      "source_type" is "WEB"
      "status" is "failed"
      "ingestion_stage" is "failed_indexing"
      "progress_percent" is 100
      "quality" is "fail"
      "selected" is state.web_selected
      "status_label" is "Failed â€¢ WEB"
      "scope_label" is state.web_primary_tag + " â€¢ priority"
      "weight" is state.web_weight
      "tags" is state.web_tags
      "primary_tag" is state.web_primary_tag
      "quality_note" is state.web_error_note
      "last_error" is "web_fetch_failed"
    as web_failed_row

    delete "Notice" where true
    create "Notice" with map:
      "id" is "current"
      "message" is "Web source fetch failed. Verify the URL and try again."
    as notice_web_fetch_failed

    let summary_status is call flow "rag_engine.refresh_index_summary":
      input:
      output:
        status
    let project_status is call flow "rag_engine.refresh_project_stats":
      input:
      output:
        status

    return map:
      "status" is "fetch_failed"

  set state.web_chunk with:
    chunk_id is state.web_upload_id + ":web:1"
    upload_id is state.web_upload_id
    document_id is state.web_upload_id
    source_name is state.web_source_name
    page_number is 1
    chunk_index is 1
    ingestion_phase is "web"
    text is state.web_body
    keywords is list: "web", "url", "source", "retrieval"

  try:
    let existing_chunks is state.index.chunks
    set state.existing_chunks is existing_chunks
  with catch err:
    set state.existing_chunks is list:

  set state.kept_chunks is list:
  for each chunk in state.existing_chunks:
    if chunk.upload_id is not state.web_upload_id:
      set state.kept_chunks is list append state.kept_chunks with chunk

  set state.kept_chunks is list append state.kept_chunks with state.web_chunk
  set state.index with:
    chunks is state.kept_chunks

  delete "DocumentLibrary" where upload_id is state.web_upload_id
  create "DocumentLibrary" with map:
    "id" is state.web_upload_id
    "upload_id" is state.web_upload_id
    "document_id" is state.web_upload_id
    "source_name" is state.web_source_name
    "source_type" is "WEB"
    "status" is "indexed"
    "ingestion_stage" is "indexed"
    "progress_percent" is 100
    "quality" is "pass"
    "selected" is state.web_selected
    "status_label" is "Indexed â€¢ WEB"
    "scope_label" is state.web_primary_tag + " â€¢ priority"
    "weight" is state.web_weight
    "tags" is state.web_tags
    "primary_tag" is state.web_primary_tag
    "quality_note" is "Web source indexed from URL."
    "last_error" is ""
  as web_indexed_row

  find "ProjectBinding" where id is state.active_project_id + "::" + state.web_upload_id
  let binding_count is list length of projectbinding_results
  if binding_count is 0:
    create "ProjectBinding" with map:
      "id" is state.active_project_id + "::" + state.web_upload_id
      "project_id" is state.active_project_id
      "upload_id" is state.web_upload_id
    as web_project_binding

  delete "Notice" where true
  create "Notice" with map:
    "id" is "current"
    "message" is "Web source indexed and ready for retrieval."
  as notice_web_indexed

  let summary_status is call flow "rag_engine.refresh_index_summary":
    input:
    output:
      status
  let project_status is call flow "rag_engine.refresh_project_stats":
    input:
    output:
      status

  return map:
    "status" is "ok"

flow "ingest_selected": requires true
  let init_status is call flow "rag_engine.initialize_defaults":
    input:
    output:
      status

  let projects_status is call flow "rag_engine.ensure_projects":
    input:
    output:
      status

  find "DocumentLibrary" where selected is true
  set state.selected_docs is list:
  find "ProjectBinding" where project_id is state.active_project_id
  set state.active_project_upload_ids is list:
  for each binding in projectbinding_results:
    set state.active_project_upload_ids is list append state.active_project_upload_ids with binding.upload_id

  set state.ingest_scope_override_input is list:
  try:
    let override_upload_ids is state.ingest_scope_override_upload_ids
    for each override_upload_id in override_upload_ids:
      if override_upload_id is not "":
        set state.ingest_scope_override_input is list append state.ingest_scope_override_input with override_upload_id
  with catch err:
    set state.ingest_scope_override_input is state.ingest_scope_override_input
  let override_upload_count is list length of state.ingest_scope_override_input

  for each doc in documentlibrary_results:
    set state.include_doc is false
    if override_upload_count is greater than 0:
      for each override_upload_id in state.ingest_scope_override_input:
        if override_upload_id is doc.upload_id:
          set state.include_doc is true
    else:
      for each active_upload_id in state.active_project_upload_ids:
        if active_upload_id is doc.upload_id:
          set state.include_doc is true
    if state.include_doc is true:
      set state.selected_docs is list append state.selected_docs with doc

  let selected_count is list length of state.selected_docs
  if selected_count is 0:
    if override_upload_count is greater than 0:
      for each doc in documentlibrary_results:
        set state.include_doc is false
        for each override_upload_id in state.ingest_scope_override_input:
          if override_upload_id is doc.upload_id:
            set state.include_doc is true
        if state.include_doc is true:
          set state.selected_docs is list append state.selected_docs with doc
      let selected_count is list length of state.selected_docs
    else:
      for each doc in documentlibrary_results:
        set state.selected_docs is list append state.selected_docs with doc
      let selected_count is list length of state.selected_docs

  if selected_count is 0:
    delete "Notice" where true
    create "Notice" with map:
      "id" is "current"
      "message" is "indexing_without_selection: Select at least one uploaded source before indexing."
    as notice_no_selection
    let summary_status is call flow "rag_engine.refresh_index_summary":
      input:
      output:
        status
    return map:
      "status" is "no_selection"

  try:
    let existing_chunks is state.index.chunks
    set state.existing_chunks is existing_chunks
  with catch err:
    set state.existing_chunks is list:

  set state.failed_count is 0
  set state.indexed_count is 0

  for each doc in state.selected_docs:
    set state.ingestion_stage is "parsing"
    set state.progress_percent is 25

    delete "DocumentLibrary" where upload_id is doc.upload_id
    create "DocumentLibrary" with map:
      "id" is doc.id
      "upload_id" is doc.upload_id
      "document_id" is doc.document_id
      "source_name" is doc.source_name
      "source_type" is doc.source_type
      "status" is "indexing"
      "ingestion_stage" is state.ingestion_stage
      "progress_percent" is state.progress_percent
      "quality" is "pending"
      "selected" is doc.selected
      "status_label" is "Parsing â€¢ " + doc.source_type
      "scope_label" is doc.primary_tag + " â€¢ priority"
      "weight" is doc.weight
      "tags" is doc.tags
      "primary_tag" is doc.primary_tag
      "quality_note" is "Parsing upload content."
      "last_error" is ""
    as parsing_row

    set state.ingestion_stage is "chunking"
    set state.progress_percent is 50

    delete "DocumentLibrary" where upload_id is doc.upload_id
    create "DocumentLibrary" with map:
      "id" is doc.id
      "upload_id" is doc.upload_id
      "document_id" is doc.document_id
      "source_name" is doc.source_name
      "source_type" is doc.source_type
      "status" is "indexing"
      "ingestion_stage" is state.ingestion_stage
      "progress_percent" is state.progress_percent
      "quality" is "pending"
      "selected" is doc.selected
      "status_label" is "Chunking â€¢ " + doc.source_type
      "scope_label" is doc.primary_tag + " â€¢ priority"
      "weight" is doc.weight
      "tags" is doc.tags
      "primary_tag" is doc.primary_tag
      "quality_note" is "Preparing deterministic chunks."
      "last_error" is ""
    as chunking_row

    set state.ingestion_stage is "embedding"
    set state.progress_percent is 75

    delete "DocumentLibrary" where upload_id is doc.upload_id
    create "DocumentLibrary" with map:
      "id" is doc.id
      "upload_id" is doc.upload_id
      "document_id" is doc.document_id
      "source_name" is doc.source_name
      "source_type" is doc.source_type
      "status" is "indexing"
      "ingestion_stage" is state.ingestion_stage
      "progress_percent" is state.progress_percent
      "quality" is "pending"
      "selected" is doc.selected
      "status_label" is "Embedding â€¢ " + doc.source_type
      "scope_label" is doc.primary_tag + " â€¢ priority"
      "weight" is doc.weight
      "tags" is doc.tags
      "primary_tag" is doc.primary_tag
      "quality_note" is "Generating deterministic embedding index."
      "last_error" is ""
    as embedding_row

    set state.ingestion_stage is "indexing"
    set state.progress_percent is 90

    delete "DocumentLibrary" where upload_id is doc.upload_id
    create "DocumentLibrary" with map:
      "id" is doc.id
      "upload_id" is doc.upload_id
      "document_id" is doc.document_id
      "source_name" is doc.source_name
      "source_type" is doc.source_type
      "status" is "indexing"
      "ingestion_stage" is state.ingestion_stage
      "progress_percent" is state.progress_percent
      "quality" is "pending"
      "selected" is doc.selected
      "status_label" is "Indexing â€¢ " + doc.source_type
      "scope_label" is doc.primary_tag + " â€¢ priority"
      "weight" is doc.weight
      "tags" is doc.tags
      "primary_tag" is doc.primary_tag
      "quality_note" is "Writing chunks to deterministic index."
      "last_error" is ""
    as indexing_row

    set state.incoming_chunks is list:
    set state.quality is "pass"
    set state.last_error is ""
    set state.ocr_retry_used is false
    set state.preview_recovery_used is false
    set state.preview_text is ""
    set state.index_payload is ""
    set state.report_payload is ""
    set state.skip_pipeline is false
    if doc.source_type is "WEB":
      set state.skip_pipeline is true
    if state.skip_pipeline is true:
      for each existing_chunk in state.existing_chunks:
        if existing_chunk.upload_id is doc.upload_id:
          set state.incoming_chunks is list append state.incoming_chunks with existing_chunk
      let web_existing_chunk_count is list length of state.incoming_chunks
      if web_existing_chunk_count is 0:
        set state.web_chunk with:
          chunk_id is doc.upload_id + ":web:1"
          upload_id is doc.upload_id
          document_id is doc.document_id
          source_name is doc.source_name
          page_number is 1
          chunk_index is 1
          ingestion_phase is "web"
          text is "Web source placeholder for " + doc.source_name + ". Re-add this URL to refresh the indexed page."
          keywords is list: "web", "url", "source", "retrieval"
        set state.incoming_chunks is list append state.incoming_chunks with state.web_chunk
    else:
      try:
        let out is call pipeline "ingestion":
          input:
            upload_id is doc.upload_id
          output:
            report
            ingestion
            index

        set state.index_payload is map get out key "index"
        set state.report_payload is map get out key "report"

        try:
          let quality_status is map get state.report_payload key "status"
          if quality_status is not "":
            set state.quality is quality_status
        with catch err:
          set state.quality is state.quality

        try:
          let chunk_list is map get state.index_payload key "chunks"
          set state.incoming_chunks is chunk_list
        with catch err:
          set state.incoming_chunks is list:
        if state.quality is "block":
          set state.incoming_chunks is list:

        set state.retry_ocr is false
        if doc.source_type is "PDF":
          if state.quality is "block":
            set state.retry_ocr is true
        if state.retry_ocr is true:
          try:
            let ocr_out is call pipeline "ingestion":
              input:
                upload_id is doc.upload_id
                mode is "ocr"
              output:
                report
                ingestion
                index
            set state.ocr_retry_used is true
            set state.index_payload is map get ocr_out key "index"
            set state.report_payload is map get ocr_out key "report"

            try:
              let ocr_quality_status is map get state.report_payload key "status"
              if ocr_quality_status is not "":
                set state.quality is ocr_quality_status
            with catch err:
              set state.quality is state.quality

            try:
              let ocr_chunk_list is map get state.index_payload key "chunks"
              set state.incoming_chunks is ocr_chunk_list
            with catch err:
              set state.incoming_chunks is list:
            if state.quality is "block":
              set state.incoming_chunks is list:
          with catch err:
            set state.last_error is "pipeline_ocr_error"
      with catch err:
        set state.quality is "fail"
        set state.last_error is "pipeline_error"
        set state.incoming_chunks is list:

    let incoming_count is list length of state.incoming_chunks
    if incoming_count is 0:
      set state.preview_text is ""
      try:
        let report_preview is map get state.report_payload key "preview"
        if report_preview is not "":
          set state.preview_text is report_preview
      with catch err:
        set state.preview_text is state.preview_text
      if state.preview_text is not "":
        set state.preview_recovery_chunk with:
          chunk_id is doc.upload_id + ":preview:1"
          upload_id is doc.upload_id
          document_id is doc.document_id
          source_name is doc.source_name
          page_number is 1
          chunk_index is 1
          ingestion_phase is "preview_recovery"
          text is state.preview_text
          keywords is list: "preview", "recovery", "document", "source"
        set state.incoming_chunks is list append state.incoming_chunks with state.preview_recovery_chunk
        set state.preview_recovery_used is true
        set state.quality is "warn"
      else:
        set state.fallback_chunk with:
          chunk_id is doc.upload_id + ":fallback:1"
          upload_id is doc.upload_id
          document_id is doc.document_id
          source_name is doc.source_name
          page_number is 1
          chunk_index is 1
          ingestion_phase is "fallback"
          text is "Fallback chunk for " + doc.source_name + ". Upload searchable/OCR text for full grounding."
          keywords is list: "fallback", "document", "source", "grounding"
        set state.incoming_chunks is list append state.incoming_chunks with state.fallback_chunk
        if state.quality is "pass":
          set state.quality is "warn"

    set state.normalized_chunks is list:
    set state.chunk_counter is 0
    for each chunk in state.incoming_chunks:
      set state.chunk_counter is state.chunk_counter + 1
      set state.chunk_id_value is doc.upload_id
      try:
        let maybe_chunk_id is chunk.chunk_id
        if maybe_chunk_id is not "":
          set state.chunk_id_value is maybe_chunk_id
      with catch err:
        set state.chunk_id_value is state.chunk_id_value
      set state.page_number_value is 1
      try:
        let maybe_page_number is chunk.page_number
        if maybe_page_number is greater than 0:
          set state.page_number_value is maybe_page_number
      with catch err:
        set state.page_number_value is state.page_number_value
      set state.chunk_index_value is state.chunk_counter
      try:
        let maybe_chunk_index is chunk.chunk_index
        if maybe_chunk_index is greater than 0:
          set state.chunk_index_value is maybe_chunk_index
      with catch err:
        set state.chunk_index_value is state.chunk_index_value
      set state.ingestion_phase_value is "indexed"
      try:
        let maybe_ingestion_phase is chunk.ingestion_phase
        if maybe_ingestion_phase is not "":
          set state.ingestion_phase_value is maybe_ingestion_phase
      with catch err:
        set state.ingestion_phase_value is state.ingestion_phase_value
      set state.text_value is "Indexed chunk for " + doc.source_name
      try:
        let maybe_text is chunk.text
        if maybe_text is not "":
          set state.text_value is maybe_text
      with catch err:
        set state.text_value is state.text_value
      set state.keywords_value is list:
        "document"
        "source"
        "evidence"
      try:
        let maybe_keywords is chunk.keywords
        set state.keywords_value is maybe_keywords
      with catch err:
        set state.keywords_value is state.keywords_value
      set state.normalized_chunk with:
        chunk_id is state.chunk_id_value
        upload_id is doc.upload_id
        document_id is doc.document_id
        source_name is doc.source_name
        page_number is state.page_number_value
        chunk_index is state.chunk_index_value
        ingestion_phase is state.ingestion_phase_value
        text is state.text_value
        keywords is state.keywords_value
      set state.normalized_chunks is list append state.normalized_chunks with state.normalized_chunk
    set state.incoming_chunks is state.normalized_chunks

    set state.kept_chunks is list:
    for each chunk in state.existing_chunks:
      if chunk.upload_id is not doc.upload_id:
        set state.kept_chunks is list append state.kept_chunks with chunk

    set state.existing_chunks is state.kept_chunks
    for each chunk in state.incoming_chunks:
      set state.existing_chunks is list append state.existing_chunks with chunk

    set state.final_status is "indexed"
    set state.final_stage is "indexed"
    set state.final_progress is 100
    set state.final_label is "Indexed â€¢ " + doc.source_type
    set state.final_note is "Indexed and ready for retrieval."

    if state.quality is "fail":
      set state.final_status is "failed"
      set state.final_stage is "failed_indexing"
      set state.final_label is "Failed - " + doc.source_type
      set state.final_note is "Ingestion failed. Retry or upload cleaner content."
      set state.failed_count is state.failed_count + 1
    else:
      if state.quality is "block":
        set state.final_label is "Needs readable text - " + doc.source_type
        set state.final_note is "No readable text was extracted (including OCR fallback). Upload a text-searchable PDF or OCR text, then re-index."
        if state.last_error is "":
          set state.last_error is "no_readable_text"
      else:
        if state.quality is "warn":
          if state.preview_recovery_used is true:
            set state.final_label is "Indexed (Preview Recovery) - " + doc.source_type
            set state.final_note is "Indexed from low-confidence preview text after quality gate block. Re-run with OCR for better grounding."
          else:
            set state.final_label is "Indexed (Fallback) - " + doc.source_type
            set state.final_note is "Fallback chunk indexed. Upload searchable/OCR text for better grounding."
      set state.indexed_count is state.indexed_count + 1

    delete "DocumentLibrary" where upload_id is doc.upload_id
    create "DocumentLibrary" with map:
      "id" is doc.id
      "upload_id" is doc.upload_id
      "document_id" is doc.document_id
      "source_name" is doc.source_name
      "source_type" is doc.source_type
      "status" is state.final_status
      "ingestion_stage" is state.final_stage
      "progress_percent" is state.final_progress
      "quality" is state.quality
      "selected" is doc.selected
      "status_label" is state.final_label
      "scope_label" is doc.primary_tag + " â€¢ priority"
      "weight" is doc.weight
      "tags" is doc.tags
      "primary_tag" is doc.primary_tag
      "quality_note" is state.final_note
      "last_error" is state.last_error
    as final_doc

  set state.index with:
    chunks is state.existing_chunks

  delete "Notice" where true
  if state.failed_count is greater than 0:
    create "Notice" with map:
      "id" is "current"
      "message" is "Indexing completed with deterministic failures. Use Retry on failed rows."
    as notice_failed
  else:
    create "Notice" with map:
      "id" is "current"
      "message" is "Indexing complete. Ask a question to inspect citations and trust."
    as notice_indexed

  let summary_status is call flow "rag_engine.refresh_index_summary":
    input:
    output:
      status

  return map:
    "status" is "ok"

flow "bulk_reindex_selected": requires true
  let projects_status is call flow "rag_engine.ensure_projects":
    input:
    output:
      status

  find "DocumentLibrary" where selected is true
  set state.selected_docs is list:
  find "ProjectBinding" where project_id is state.active_project_id
  set state.active_project_upload_ids is list:
  for each binding in projectbinding_results:
    set state.active_project_upload_ids is list append state.active_project_upload_ids with binding.upload_id

  for each doc in documentlibrary_results:
    set state.include_doc is false
    for each active_upload_id in state.active_project_upload_ids:
      if active_upload_id is doc.upload_id:
        set state.include_doc is true
    if state.include_doc is true:
      set state.selected_docs is list append state.selected_docs with doc

  let selected_count is list length of state.selected_docs
  if selected_count is 0:
    delete "Notice" where true
    create "Notice" with map:
      "id" is "current"
      "message" is "indexing_without_selection: Select at least one source in the active project before re-index."
    as notice_bulk_reindex_empty
    let summary_status is call flow "rag_engine.refresh_index_summary":
      input:
      output:
        status
    return "no_selection"

  for each doc in state.selected_docs:
    delete "DocumentLibrary" where upload_id is doc.upload_id
    create "DocumentLibrary" with map:
      "id" is doc.id
      "upload_id" is doc.upload_id
      "document_id" is doc.document_id
      "source_name" is doc.source_name
      "source_type" is doc.source_type
      "status" is "uploaded"
      "ingestion_stage" is "queued"
      "progress_percent" is 0
      "quality" is "pending"
      "selected" is doc.selected
      "status_label" is "Queued â€¢ " + doc.source_type
      "scope_label" is doc.primary_tag + " â€¢ priority"
      "weight" is doc.weight
      "tags" is doc.tags
      "primary_tag" is doc.primary_tag
      "quality_note" is "Queued for deterministic re-index."
      "last_error" is ""
    as queued_doc

  delete "Notice" where true
  create "Notice" with map:
    "id" is "current"
    "message" is "Selected sources queued. Run Create index to execute deterministic ingestion stages."
  as notice_bulk_reindex

  let summary_status is call flow "rag_engine.refresh_index_summary":
    input:
    output:
      status

  return "ok"

flow "ingest_document": requires true
  try:
    let upload_id is input.row.upload_id
  with catch err:
    return "missing_document"

  find "DocumentLibrary" where upload_id is upload_id
  let doc_count is list length of documentlibrary_results
  if doc_count is 0:
    return "not_found"

  let doc is list get documentlibrary_results at 0

  delete "DocumentLibrary" where upload_id is upload_id
  create "DocumentLibrary" with map:
    "id" is doc.id
    "upload_id" is doc.upload_id
    "document_id" is doc.document_id
    "source_name" is doc.source_name
    "source_type" is doc.source_type
    "status" is "uploaded"
    "ingestion_stage" is "queued"
    "progress_percent" is 0
    "quality" is "pending"
    "selected" is true
    "status_label" is "Queued â€¢ " + doc.source_type
    "scope_label" is doc.primary_tag + " â€¢ priority"
    "weight" is doc.weight
    "tags" is doc.tags
    "primary_tag" is doc.primary_tag
    "quality_note" is "Queued for row re-index."
    "last_error" is ""
  as queued_doc

  delete "Notice" where true
  create "Notice" with map:
    "id" is "current"
    "message" is "Document queued. Click Create index to run deterministic ingestion."
  as notice_row_reindex

  let summary_status is call flow "rag_engine.refresh_index_summary":
    input:
    output:
      status

  return "ok"

flow "retry_document": requires true
  try:
    let upload_id is input.row.upload_id
  with catch err:
    return "missing_document"

  find "DocumentLibrary" where upload_id is upload_id
  let doc_count is list length of documentlibrary_results
  if doc_count is 0:
    return "not_found"

  let doc is list get documentlibrary_results at 0

  delete "DocumentLibrary" where upload_id is upload_id
  create "DocumentLibrary" with map:
    "id" is doc.id
    "upload_id" is doc.upload_id
    "document_id" is doc.document_id
    "source_name" is doc.source_name
    "source_type" is doc.source_type
    "status" is "uploaded"
    "ingestion_stage" is "queued"
    "progress_percent" is 0
    "quality" is "pending"
    "selected" is true
    "status_label" is "Queued â€¢ " + doc.source_type
    "scope_label" is doc.primary_tag + " â€¢ priority"
    "weight" is doc.weight
    "tags" is doc.tags
    "primary_tag" is doc.primary_tag
    "quality_note" is "Retry queued for deterministic ingestion."
    "last_error" is ""
  as retry_doc

  delete "Notice" where true
  create "Notice" with map:
    "id" is "current"
    "message" is "Retry queued. Click Create index to run deterministic stages again."
  as notice_retry

  let summary_status is call flow "rag_engine.refresh_index_summary":
    input:
    output:
      status

  return "ok"

